{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e6c2049-66a2-45b4-9287-47d06ec32d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed-form solution estimates: [ 1.49953174 -0.99953355 -0.24970758  0.74950415  3.49957802 -1.99918604\n",
      "  0.49960829  0.99885954  1.25101421  1.99884709]\n",
      "Gradient Descent estimates: [ 0.00903862 -0.00591682 -0.00138371  0.0045886   0.0207754  -0.01192839\n",
      "  0.00301619  0.00586153  0.00760967  0.01184054]\n",
      "L-BFGS-B algorithm estimates: [ 1.49953174 -0.99953355 -0.24970759  0.74950412  3.49957802 -1.99918604\n",
      "  0.4996083   0.99885952  1.25101423  1.99884707]\n",
      "Nelder-Mead algorithm estimates: [ 0.76008281 -1.23992452 -1.00766952  0.40008519  2.67700223 -2.60819741\n",
      " -0.24136479 -0.71132415  1.08268135  1.50808676]\n",
      "Beta_MLE_LBFGSB: [ 1.5  -1.   -0.25  0.75  3.5  -2.    0.5   1.    1.25  2.  ]\n",
      "Beta_MLE_LBFGSB: [ 1.5  -1.   -0.25  0.75  3.5  -2.    0.5   1.    1.25  2.  ]\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.997\n",
      "Model:                            OLS   Adj. R-squared:                  0.997\n",
      "Method:                 Least Squares   F-statistic:                 4.375e+06\n",
      "Date:                Sat, 30 Mar 2024   Prob (F-statistic):               0.00\n",
      "Time:                        23:23:44   Log-Likelihood:                -3080.3\n",
      "No. Observations:              100000   AIC:                             6181.\n",
      "Df Residuals:                   99990   BIC:                             6276.\n",
      "Df Model:                           9                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          1.4995      0.001   1900.039      0.000       1.498       1.501\n",
      "x1            -0.9995      0.001  -1261.812      0.000      -1.001      -0.998\n",
      "x2            -0.2497      0.001   -315.467      0.000      -0.251      -0.248\n",
      "x3             0.7495      0.001    948.510      0.000       0.748       0.751\n",
      "x4             3.4996      0.001   4425.582      0.000       3.498       3.501\n",
      "x5            -1.9992      0.001  -2535.789      0.000      -2.001      -1.998\n",
      "x6             0.4996      0.001    630.970      0.000       0.498       0.501\n",
      "x7             0.9989      0.001   1263.691      0.000       0.997       1.000\n",
      "x8             1.2510      0.001   1582.645      0.000       1.249       1.253\n",
      "x9             1.9988      0.001   2529.516      0.000       1.997       2.000\n",
      "==============================================================================\n",
      "Omnibus:                        1.330   Durbin-Watson:                   2.001\n",
      "Prob(Omnibus):                  0.514   Jarque-Bera (JB):                1.321\n",
      "Skew:                          -0.008   Prob(JB):                        0.517\n",
      "Kurtosis:                       3.006   Cond. No.                         1.02\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(100)\n",
    "\n",
    "# Constants\n",
    "N = 100000  # Number of rows\n",
    "K = 10      # Number of columns\n",
    "sigma = 0.5 # Standard deviation for eps\n",
    "\n",
    "# Creating the matrix X\n",
    "X = np.random.randn(N, K)\n",
    "X[:, 0] = 1  # Set the first column to 1's\n",
    "\n",
    "# Creating the vector eps\n",
    "eps = np.random.normal(0, sigma**2, N)\n",
    "\n",
    "# Creating the vector beta\n",
    "beta = np.array([1.5, -1, -0.25, 0.75, 3.5, -2, 0.5, 1, 1.25, 2])\n",
    "\n",
    "# Generating Y\n",
    "Y = X.dot(beta) + eps\n",
    "\n",
    "# Part 2: Computing beta_OLS using the closed-form solution\n",
    "beta_OLS_closed_form = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)\n",
    "\n",
    "# Part 3: Gradient Descent Implementation\n",
    "def gradient_descent(X, Y, learning_rate=0.0000003, iterations=10000):\n",
    "    beta = np.zeros(X.shape[1])\n",
    "    n = float(len(Y))\n",
    "    for i in range(iterations):\n",
    "        Y_pred = X.dot(beta)\n",
    "        gradient = -(2/n) * X.T.dot(Y - Y_pred)\n",
    "        beta -= learning_rate * gradient\n",
    "    return beta\n",
    "\n",
    "# Reduced iterations for demonstration purposes\n",
    "beta_OLS_gradient_descent = gradient_descent(X, Y)\n",
    "\n",
    "# Part 4: Optimization using L-BFGS-B and Nelder-Mead\n",
    "def objective_function(beta, X, Y):\n",
    "    residuals = Y - X.dot(beta)\n",
    "    return np.sum(residuals**2)\n",
    "\n",
    "initial_beta = np.zeros(K)\n",
    "result_L_BFGS_B = minimize(fun=objective_function, x0=initial_beta, args=(X, Y), method='L-BFGS-B')\n",
    "result_Nelder_Mead = minimize(fun=objective_function, x0=initial_beta, args=(X, Y), method='Nelder-Mead')\n",
    "\n",
    "# Display results\n",
    "print(\"Closed-form solution estimates:\", beta_OLS_closed_form)\n",
    "print(\"Gradient Descent estimates:\", beta_OLS_gradient_descent)\n",
    "print(\"L-BFGS-B algorithm estimates:\", result_L_BFGS_B.x)\n",
    "print(\"Nelder-Mead algorithm estimates:\", result_Nelder_Mead.x)\n",
    "\n",
    "\n",
    "\n",
    "# Assuming X, Y, and beta are already defined from previous steps\n",
    "\n",
    "# Provided gradient function for MLE estimation\n",
    "def gradient(theta, Y, X):\n",
    "    grad = np.zeros(len(theta))\n",
    "    beta = theta[:-1]\n",
    "    sigma = theta[-1]\n",
    "    grad[:-1] = -X.T @ (Y - X @ beta) / (sigma ** 2)\n",
    "    grad[-1] = len(Y) / sigma - np.sum((Y - X @ beta) ** 2) / (sigma ** 3)\n",
    "    return grad\n",
    "\n",
    "# Step 5: Define the objective function for MLE and use the provided gradient function\n",
    "def log_likelihood(theta, Y, X):\n",
    "    beta = theta[:-1]\n",
    "    sigma = theta[-1]\n",
    "    n = len(Y)\n",
    "    residuals = Y - X.dot(beta)\n",
    "    return -n/2 * np.log(2 * np.pi) - n * np.log(sigma) - 1/(2*sigma**2) * np.sum(residuals**2)\n",
    "\n",
    "# Initial guess for theta (beta coefficients and sigma)\n",
    "initial_theta = np.append(beta, sigma)\n",
    "\n",
    "# Minimize negative log likelihood\n",
    "result_MLE = minimize(fun=lambda theta: -log_likelihood(theta, Y, X),\n",
    "                      x0=initial_theta, method='L-BFGS-B', jac=lambda theta: -gradient(theta, Y, X))\n",
    "beta_MLE_LBFGSB = result_MLE.x[:-1]\n",
    "\n",
    "# Display the MLE estimates for comparison\n",
    "print(\"Beta_MLE_LBFGSB:\", beta_MLE_LBFGSB)\n",
    "# Step 6: Compute beta_OLS using statsmodels\n",
    "model = sm.OLS(Y, X).fit()\n",
    "print(\"Beta_MLE_LBFGSB:\", beta_MLE_LBFGSB)\n",
    "# Print the summary\n",
    "print(model.summary())\n",
    "\n",
    "# Step 7: Export the summary to .tex file\n",
    "with open('regression_summary.tex', 'w') as file:\n",
    "    file.write(model.summary().as_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03b53d2-4d53-467b-ba06-ef6a248e9f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5530611c-65b1-4e26-a337-b116d1f0f5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f92f2c-f96d-4349-aea8-da3619402de4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17236bf0-a725-4945-a59d-1054b90af298",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
